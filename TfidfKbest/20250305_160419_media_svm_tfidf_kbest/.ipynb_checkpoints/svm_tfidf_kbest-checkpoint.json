{
  "experiment_setup": {
    "config": {
      "classifier_type": "svm",
      "embedding_type": "tfidf",
      "brand_removal": false,
      "use_select_kbest": true,
      "use_smote": false,
      "use_lexical_syntactic": false,
      "use_vader_sentiment": false,
      "use_lda": false,
      "save_interesting_articles": false,
      "use_scaler": false,
      "use_bias_features": false,
      "lexical_csv_path": "precomputed_lexical_default.csv",
      "sentiment_csv_path": "precomputed_sentiment_default.csv",
      "bias_csv_path": "precomputed_bias_default.csv"
    },
    "best_params": {
      "clf__C": 1.7965626423790628,
      "clf__class_weight": null,
      "clf__kernel": "linear",
      "clf__max_iter": 500,
      "features__text__vectorizer__max_df": 0.8,
      "features__text__vectorizer__max_features": 10000,
      "features__text__vectorizer__min_df": 1,
      "features__text__vectorizer__ngram_range": [
        1,
        2
      ],
      "select__k": 1815,
      "select__score_func": "chi2"
    },
    "best_cv_score": 0.5459004556398901,
    "split_mode": "media"
  },
  "performance_summary": {
    "test_metrics": {
      "accuracy": 0.4969230769230769,
      "mae": 0.7715384615384615,
      "macro_f1": 0.3785568596240667,
      "macro_precision": 0.4637583916575087,
      "macro_recall": 0.40882059534063364,
      "weighted_f1": 0.438354515360412,
      "weighted_precision": 0.4747858185519829,
      "weighted_recall": 0.4969230769230769
    },
    "test_confusion_matrix": [
      [
        142,
        15,
        245
      ],
      [
        77,
        19,
        203
      ],
      [
        104,
        10,
        485
      ]
    ],
    "test_confidence_mean": 0.6985418842157609,
    "test_confidence_std": 0.2831011851945296,
    "subgroup_source": {},
    "subgroup_topic": {
      "media_bias": {
        "accuracy": 0.6616541353383458,
        "macro_f1": 0.3106575963718821,
        "num_samples": 133
      },
      "culture": {
        "accuracy": 0.0,
        "macro_f1": 0.0,
        "num_samples": 5
      },
      "federal_budget": {
        "accuracy": 0.3137254901960784,
        "macro_f1": 0.18468468468468469,
        "num_samples": 51
      },
      "opioid_crisis": {
        "accuracy": 0.25,
        "macro_f1": 0.16666666666666666,
        "num_samples": 4
      },
      "politics": {
        "accuracy": 0.5383386581469649,
        "macro_f1": 0.42976544586974036,
        "num_samples": 626
      },
      "education": {
        "accuracy": 0.6206896551724138,
        "macro_f1": 0.3409090909090909,
        "num_samples": 29
      },
      "race_and_racism": {
        "accuracy": 0.32075471698113206,
        "macro_f1": 0.2539423449344326,
        "num_samples": 106
      },
      "environment": {
        "accuracy": 0.25,
        "macro_f1": 0.14814814814814814,
        "num_samples": 8
      },
      "lgbt_rights": {
        "accuracy": 0.6027397260273972,
        "macro_f1": 0.3725055432372506,
        "num_samples": 73
      },
      "fbi": {
        "accuracy": 0.5571428571428572,
        "macro_f1": 0.3693398799781779,
        "num_samples": 70
      },
      "gun_control_and_gun_rights": {
        "accuracy": 0.5,
        "macro_f1": 0.3333333333333333,
        "num_samples": 12
      },
      "national_defense": {
        "accuracy": 0.3055555555555556,
        "macro_f1": 0.21212121212121213,
        "num_samples": 36
      },
      "cybersecurity": {
        "accuracy": 0.5833333333333334,
        "macro_f1": 0.338235294117647,
        "num_samples": 24
      },
      "supreme_court": {
        "accuracy": 0.3333333333333333,
        "macro_f1": 0.2930756843800322,
        "num_samples": 21
      },
      "labor": {
        "accuracy": 0.3,
        "macro_f1": 0.16666666666666666,
        "num_samples": 10
      },
      "justice_department": {
        "accuracy": 0.5714285714285714,
        "macro_f1": 0.24242424242424243,
        "num_samples": 14
      },
      "us_military": {
        "accuracy": 0.16666666666666666,
        "macro_f1": 0.1111111111111111,
        "num_samples": 6
      },
      "nuclear_weapons": {
        "accuracy": 0.09523809523809523,
        "macro_f1": 0.057971014492753624,
        "num_samples": 21
      },
      "republican_party": {
        "accuracy": 0.6153846153846154,
        "macro_f1": 0.38095238095238093,
        "num_samples": 13
      },
      "palestine": {
        "accuracy": 0.14285714285714285,
        "macro_f1": 0.08333333333333333,
        "num_samples": 7
      },
      "polarization": {
        "accuracy": 0.25,
        "macro_f1": 0.13333333333333333,
        "num_samples": 4
      },
      "banking_and_finance": {
        "accuracy": 0.0,
        "macro_f1": 0.0,
        "num_samples": 8
      },
      "great_britain": {
        "accuracy": 0.5,
        "macro_f1": 0.25,
        "num_samples": 6
      },
      "justice": {
        "accuracy": 0.0,
        "macro_f1": 0.0,
        "num_samples": 4
      },
      "trade": {
        "accuracy": 0.0,
        "macro_f1": 0.0,
        "num_samples": 1
      },
      "treasury": {
        "accuracy": 0.3333333333333333,
        "macro_f1": 0.27777777777777773,
        "num_samples": 6
      },
      "mexico": {
        "accuracy": 0.0,
        "macro_f1": 0.0,
        "num_samples": 2
      }
    }
  },
  "timing": {
    "run_time_seconds": 681.1874074935913,
    "run_time_minutes": 11.353123458226522
  },
  "analysis_info": {
    "train_class_distribution": {
      "2": 10241,
      "0": 8861,
      "1": 7488
    },
    "test_class_distribution": {
      "2": 599,
      "0": 402,
      "1": 299
    },
    "pred_class_distribution": {
      "2": 933,
      "0": 323,
      "1": 44
    },
    "class_0_avg_probability": 0.11569841658653444,
    "class_1_avg_probability": 0.0931744217676101,
    "class_2_avg_probability": 0.7911271616458554,
    "svm_num_support_vectors": 2755
  }
}