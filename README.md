# ğŸ“° Article-Level Political Bias Classification (AllSides) â€” Results

This repository contains the **results** from my dissertation on detecting and classifying political bias in news articles (**Left / Center / Right**). I compare **traditional ML** (NaÃ¯ve Bayes, Logistic Regression, SVM, Random Forest, XGBoost) against **transformer models** (e.g., BERT family) on the AllSides corpus (~37k articles).

**In short:** classical models with TF-IDF plus features like topics, sentiment, and lexical cues are competitive, but **BERT/RoBERTa** achieve the strongest **in-domain** (random-split) macro-F1. Performance **drops on cross-publisher** (media-split) tests unless **brand mentions are removed** and/or additional linguistic cues are leveraged. The results highlight the importance of **data quality**, **feature design**, and **domain-shift evaluation** for any real-world deployment.

---

# ğŸ“ Whatâ€™s inside 

Each folder groups JSON run artifacts (configs, metrics, confusion matrices) for a theme or ablation:

* [`countVsTfidf/`](./countVsTfidf) â€“ Count vs TF-IDF baselines
* [`TfidfKbest/`](./TfidfKbest) â€“ TF-IDF + SelectKBest
* [`smote/`](./smote) â€“ Class-balancing (SMOTE)
* [`scaler/`](./scaler) â€“ Feature scaling variants
* [`lda/`](./lda) â€“ Topic features (LDA)
* [`vader/`](./vader) â€“ Sentiment features (VADER)
* [`lexical/`](./lexical) â€“ Lexical / empirical cues (e.g., POS ratios, hedges)
* [`biasfeatures/`](./biasfeatures) â€“ Bias-lexicon variants
* [`brandremoval/`](./brandremoval) â€“ Removing publisher/brand mentions
* [`combinationexperiments/`](./combinationexperiments), [`combinationexperiments2/`](./combinationexperiments2) â€“ Combined feature settings & ablations.
